---
title: "Capstone Milestone Report"
author: "Paul Hickman"
output:
  html_document:
    keep_md: no
  pdf_document:
    keep_tex: no
---
```{r global_options, include = FALSE}
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE)
```

# Overview
This is the milestone report for the Data Science Specialisation Capstone unit.  It documents the method to load and analyse the supplied text files using R's tm module for text mining.  The final product will be a system that can predict the next word in a sentence.  This report provides the initial analysis and suggests a direction for the future system.

This report's goal is to demonstrate I have gotten used to working with the data and I am on track to create a prediction algorithm.  It explains my exploratory analysis and goals for the eventual app and algorithm.  This document explains only the major features of the data and briefly summarize my plans for creating the prediction algorithm and Shiny app.

# Data
The analysis uses data from news headlines, blog entries, and tweets.  Only the English language data is used.

* Download data from [here](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip)


```{r echo=FALSE}
#setwd("C:/Users/phickma/Desktop/GitHub/Data Science Capstone")
setwd('C:/Users/paul_/Documents/Code/Data-Science-Capstone')

# load the required libraries
for(package in c("tm", "NLP", "stringr", "RWeka", "colorspace", "ggplot2", "knitr","rmarkdown", "slam", "ggplot2")) {
    if(!require(package, character.only = TRUE)) {
        install.packages(package)
        library(package, character.only = TRUE)
    }
}
```

# Loading the Data

Load the three data files and display information about the size.

```{r}
# load all the data
twitter.data <- readLines(file.path("final","en_US","en_US.twitter.txt"), encoding = "UTF-8")
blogs.data <- readLines(file.path("final","en_US","en_US.blogs.txt"), encoding = "UTF-8")
news.data <- readLines(file.path("final","en_US","en_US.news.txt"), encoding = "UTF-8")
```

Detail some statistics about the data.

```{r echo=FALSE}
temp <- matrix(c(
    "en_US.twitter.txt", "en_US.news.txt", "en_US.blogs.txt",
    round(file.info("final/en_US/en_US.twitter.txt")$size/1024^2,2), round(file.info("final/en_US/en_US.news.txt")$size/1024^2, 2), round(file.info("final/en_US/en_US.blogs.txt")$size/1024^2, 2),
    length(twitter.data), length(news.data), length(blogs.data),
    max(nchar(twitter.data)), max(nchar(news.data)), max(nchar(blogs.data)),
    30451170, 34494539, 37570839
    ),
    ncol = 5, byrow = FALSE)

colnames(temp) <- c("Filename", "File Size (mb)", "# of Lines", "Longest Line (# chars)", "# of Words")

kable(temp, format = "markdown")
```

# Sampling the Data

The raw data is too large to process easily on a standard computer.  To improve the efficiency of data processing a random sample of each dataset is used.

```{r}
# take a sample of the raw data
set.seed(982735)
twitter.sample <- twitter.data[sample(1:length(twitter.data), length(twitter.data) * 0.01)]
blogs.sample <- blogs.data[sample(1:length(blogs.data), length(blogs.data) * 0.01)]
news.sample <- news.data[sample(1:length(news.data), length(news.data) * 0.01)]

# combine the data into one
all.sample <- c(twitter.sample, blogs.sample, news.sample)

rm(twitter.data, blogs.data, news.data)
```
```{r include=FALSE}
#twitter.sample <- twitter.data[1:10]
#blogs.sample <- twitter.data[1:10]
#news.sample <- twitter.data[1:10]
```

# Cleaning the Data

The data contains characters that negatively impact analysis.  The following changes will be made to the data:
* Convert to ASCII
* Remove punctuation
* Remove numbers
* Convert to lower case
* Remove excess white spaces

```{r}
all.docs <- Corpus(VectorSource(all.sample))
rm(all.sample)

# Preprocessing
all.docs <- tm_map(all.docs, content_transformer(function(x) iconv(x, from = "UTF-8", to = "ASCII", sub = "")))
all.docs <- tm_map(all.docs, removePunctuation)
all.docs <- tm_map(all.docs, removeNumbers)
all.docs <- tm_map(all.docs, tolower)

# optional:
#all.docs <- tm_map(all.docs, removeWords, stopwords("english"))
# This is referred to as 'stemming' documents. We stem the documents so that a word will be recognizable to the computer, despite whether or not it may have a variety of possible endings in the original text.
#library(SnowballC)
#all.docs <- tm_map(all.docs, stemDocument)   

# To Finish
all.docs <- tm_map(all.docs, stripWhitespace)
# Be sure to use the following script once you have completed preprocessing.
# This tells R to treat your preprocessed documents as text documents.
all.docs <- tm_map(all.docs, PlainTextDocument)
```

# Analysing the Data

The data is now in a state where n-grams can be calculated.  The analysis will determine the highest frequency uni, bi and tri ngrams.  These are the most frequently used words in the dataset and will be of importance in the future prediction model.

## Uni-grams

```{r}
unigram.tokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
unigram.tdm <- TermDocumentMatrix(all.docs, control = list(tokenize = unigram.tokenizer))

unigram.freq <- rowapply_simple_triplet_matrix(unigram.tdm, sum)
uniword.freq_data <- data.frame(ngram = names(unigram.freq), freq = unigram.freq, row.names = NULL, stringsAsFactors = FALSE)
uniword.freq_data <- uniword.freq_data[with(uniword.freq_data, order(-freq, ngram)), ]

ggplot(
    head(uniword.freq_data, 20), 
    aes(x = reorder(ngram,freq,function(x) -x), freq)
    ) + 
    geom_bar(stat='identity', fill="steelblue") + 
    geom_text(aes(label=freq), vjust=1.6, color="white", size=2.5) +
    ggtitle("Top 20 Unigrams") +
    xlab("n-gram") + ylab("Frequency") +
    theme(axis.text.x = element_text(angle = 45))
```
```{r echo=FALSE}
# save the word frequency for later
save(uniword.freq_data, file = "data/uniword.freq_data.RData")
rm(unigram.tokenizer, unigram.tdm, unigram.freq, uniword.freq_data)
```

## Bi-grams

```{r}
bigram.tokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
bigram.tdm <- TermDocumentMatrix(all.docs, control = list(tokenize = bigram.tokenizer))

bigram.freq <- rowapply_simple_triplet_matrix(bigram.tdm, sum)
biword.freq_data <- data.frame(ngram = names(bigram.freq), freq = bigram.freq, row.names = NULL, stringsAsFactors = FALSE)
biword.freq_data <- biword.freq_data[with(biword.freq_data, order(-freq, ngram)), ]

ggplot(
    head(biword.freq_data, 20), 
    aes(x = reorder(ngram,freq,function(x) -x), freq)
    ) + 
    geom_bar(stat='identity', fill="steelblue") + 
    geom_text(aes(label=freq), vjust=1.6, color="white", size=3) +
    ggtitle("Top 20 Bigrams") +
    xlab("n-gram") + ylab("Frequency") +
    theme(axis.text.x = element_text(angle = 45))
```
```{r echo=FALSE}
# save the word frequency for later
save(biword.freq_data, file = "data/biword.freq_data.RData")
rm(bigram.tokenizer, bigram.tdm, bigram.freq, biword.freq_data)
```

## Tri-grams

```{r}
trigram.tokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
trigram.tdm <- TermDocumentMatrix(all.docs, control = list(tokenize = trigram.tokenizer))

trigram.freq <- rowapply_simple_triplet_matrix(trigram.tdm, sum)
triword.freq_data <- data.frame(ngram = names(trigram.freq), freq = trigram.freq, row.names = NULL, stringsAsFactors = FALSE)
triword.freq_data <- triword.freq_data[with(triword.freq_data, order(-freq, ngram)), ]

ggplot(
    head(triword.freq_data, 20), 
    aes(x = reorder(ngram,freq,function(x) -x), freq)
    ) + 
    geom_bar(stat='identity', fill="steelblue") + 
    geom_text(aes(label=freq), vjust=1.6, color="white", size=3) +
    ggtitle("Top 20 Trigrams") +
    xlab("n-gram") + ylab("Frequency") +
    theme(axis.text.x = element_text(angle = 90))
```
```{r echo=FALSE}
# save the word frequency for later
save(triword.freq_data, file = "data/triword.freq_data.RData")
rm(trigram.tokenizer, trigram.tdm, trigram.freq, triword.freq_data)
```

## Quad-grams

```{r}
quadgram.tokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))
quadgram.tdm <- TermDocumentMatrix(all.docs, control = list(tokenize = quadgram.tokenizer))

quadgram.freq <- rowapply_simple_triplet_matrix(quadgram.tdm, sum)
quadword.freq_data <- data.frame(ngram = names(quadgram.freq), freq = quadgram.freq, row.names = NULL, stringsAsFactors = FALSE)
quadword.freq_data <- quadword.freq_data[with(quadword.freq_data, order(-freq, ngram)), ]

ggplot(
    head(quadword.freq_data, 20), 
    aes(x = reorder(ngram,freq,function(x) -x), freq)
    ) + 
    geom_bar(stat='identity', fill="steelblue") + 
    geom_text(aes(label=freq), vjust=1.6, color="white", size=3) +
    ggtitle("Top 20 quadgrams") +
    xlab("n-gram") + ylab("Frequency") +
    theme(axis.text.x = element_text(angle = 90))
```
```{r echo=FALSE}
# save the word frequency for later
save(quadword.freq_data, file = "data/quadword.freq_data.RData")
rm(quadgram.tokenizer, quadgram.tdm, quadgram.freq, quadword.freq_data)
```

# Summary

The analysis performed relied on a small sample size to be able to complete the processing in a reasonable amount of time.  The results of the uni and bi-gram analysis shows what I would expect to be the most popular words or word combinations.  The tri-gram analysis shows 'thanks for the' is the most frequently used three-word combination.  I am interested in the optimal sample size to ensure the prediction is useful.

# Future Application

The future application will use the most common word combinations, most likely bi and tri ngrams (or more), to calculate the probability of the next word.  I will need to perform more analysis to determine if higher order ngrams are required.  I expect the final application to attempt to predict the most likely word based on the longest ngram available and, if not found, use the next longest (Katz's back-off model).  I expect the longer the ngram the higher the probability of successfully predicting the next word.  The amount of RAM required and time to predict the next word will be critical to create a useable application.
